{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MumD4hxAiM1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57516fe4-7e13-454e-8802-708ca0a4dbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQwkesy412Nu",
        "outputId": "cff2070a-0f85-4dd4-b92c-ec5d9797a279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in data and convert to a list of sentences and a list of words\n",
        "f = open('/content/drive/MyDrive/italy.txt', 'r', errors = 'ignore')\n",
        "\n",
        "raw = f.read()\n",
        "\n",
        "raw= raw.lower() #Converts to lowercase\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(raw) #Converts to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw) #Converts to list of words"
      ],
      "metadata": {
        "id": "aLhXEIXOj8oR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42291560-fdae-4954-9553-023b29e23ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens[:2]\n",
        "\n",
        "word_tokens[:2]"
      ],
      "metadata": {
        "id": "qh0L7vTekenh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6061234f-9da1-4468-b884-b537705a768a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['italy', '(']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-processing raw text; input the tokens and return normalized tokens\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "L7lS3V5NkoOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function for a greeting by the bot\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hi there\", \"what's up\", \"hey\",)\n",
        "\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad you are talking to me\"]\n",
        "\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREETING_INPUTS:\n",
        "      return random.choice(GREETING_RESPONSES)"
      ],
      "metadata": {
        "id": "a1rKh0vVliBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a response using document similarity; convert a collection of raw documents into a matrix of TF-IDF features\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "4xPY7ftzmX6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for the response\n",
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "metadata": {
        "id": "D4PVc2Tj93Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print(\"Welcome to the Travel Italy Chatbot. If you want to exit, type Bye!\")\n",
        "\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Answer: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"Answer: \"+greeting(user_response))\n",
        "            else:\n",
        "                sent_tokens.append(user_response)\n",
        "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "                final_words=list(set(word_tokens))\n",
        "                print(\"Answer: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Answer: Bye! take care..\")   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubgtzbTI-IrG",
        "outputId": "3a965bc7-bf46-4fb4-d34e-c61e078cec6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Travel Italy Chatbot. If you want to exit, type Bye!\n",
            "What kind of food is there in Italy?\n",
            "Answer: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the country is well known for its creative and innovative business,[279] a large and competitive agricultural sector[280] (with the world's largest wine production),[281] and for its influential and high-quality automobile, machinery, food, design and fashion industry.\n",
            "What are popular cities to visit?\n",
            "Answer: the most popular sport in italy is football.\n",
            "What is the weather link in Italy?\n",
            "Answer: [393]\n",
            "demographics\n",
            "main article: demographics of italy\n",
            "see also: italians, italian diaspora, genetic history of italy, list of cities in italy, and racism in italy\n",
            "map of italy's population density at the 2011 census\n",
            "\n",
            "at the beginning of 2020, italy had 60,317,116 inhabitants.\n",
            "What is there for tourists to do in Italy?\n",
            "Answer: tourism\n",
            "main article: tourism in italy\n",
            "the amalfi coast is one of italy's major tourist destinations.\n",
            "What kind of architecture does Italy have?\n",
            "Answer: roman painting does have its own unique characteristics.\n",
            "bye\n",
            "Answer: Bye! take care..\n"
          ]
        }
      ]
    }
  ]
}